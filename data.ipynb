{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import spacy\n",
    "#spacy_en = spacy.load('en')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_batch(ids, x, y):\n",
    "    batch_x = np.row_stack( [ [x[i]] for i in ids ] )\n",
    "    batch_y = np.array( [ y[i] for i in ids ] )\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "# shuffle training examples and create mini-batches\n",
    "def create_batches(x, y, batch_size, shuffle = True):\n",
    "    \n",
    "    perm = list(range(len(y)))\n",
    "    if shuffle:\n",
    "        random.shuffle(perm)\n",
    "\n",
    "    # sort sequences based on their length\n",
    "    # permutation is necessary if we want different batches every epoch\n",
    "    lst = sorted(perm, key=lambda i: len(x[i][0]))\n",
    "    #print(lst)\n",
    "    batches_x = [ ]\n",
    "    batches_y = [ ]\n",
    "    size = batch_size\n",
    "    ids = [ lst[0] ]\n",
    "    for i in lst[1:]:\n",
    "        if len(ids) < size and len(x[i][0]) == len(x[ids[0]][0]):\n",
    "            ids.append(i)\n",
    "        else:\n",
    "            bx, by = create_one_batch(ids, x, y)\n",
    "            batches_x.append(bx)\n",
    "            batches_y.append(by)\n",
    "            ids = [ i ]\n",
    "    bx, by = create_one_batch(ids, x, y)\n",
    "    batches_x.append(bx)\n",
    "    batches_y.append(by)\n",
    "\n",
    "    # shuffle batches\n",
    "    batch_perm = list(range(len(batches_x)))\n",
    "    random.shuffle(batch_perm)\n",
    "    batches_x = [ batches_x[i] for i in batch_perm ]\n",
    "    batches_y = [ batches_y[i] for i in batch_perm ]\n",
    "    return batches_x, batches_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    #return \" \".join([tok.text for tok in spacy_en.tokenizer(text)])\n",
    "    #print(text)\n",
    "    #print(word_tokenize(text))\n",
    "    return \" \".join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(datas, test_rate = 0.2):\n",
    "    random.shuffle(datas)\n",
    "    pivot_index = int(len(datas) * (1-test_rate))\n",
    "    return datas[:pivot_index], datas[pivot_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1 = '<e1>.+</e1>'\n",
    "p2 = '<e2>.+</e2>'\n",
    "\n",
    "def SemEval10_task8(sub_path, path = './dataset/SemEval2010_task8_all_data/'):\n",
    "    full_path = path + sub_path\n",
    "    \n",
    "    fin = open(full_path, 'r')\n",
    "    lines = fin.readlines()\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in range(0, len(lines), 4):\n",
    "        data = {'content': [],\n",
    "                 'category': []}\n",
    "        \n",
    "        content_line = lines[i].strip()\n",
    "        if not content_line:\n",
    "            break\n",
    "        _, raw_sentence = content_line.split('\\t')\n",
    "        raw_sentence = raw_sentence[1:-1]\n",
    "        \n",
    "        #print(raw_sentence)\n",
    "        #\n",
    "        e1_span = re.search(p1, raw_sentence).span()\n",
    "        e1 = raw_sentence[e1_span[0]+4:e1_span[1]-5]\n",
    "        \n",
    "        \n",
    "        e2_span = re.search(p2, raw_sentence).span()\n",
    "        e2 = raw_sentence[e2_span[0]+4:e2_span[1]-5]\n",
    "        \n",
    "        temp_sentence = re.sub(p1, ' E1 ', raw_sentence)\n",
    "        temp_sentence = re.sub(p2, ' E2 ', temp_sentence)\n",
    "        temp_sentence = re.sub('  ', ' ', temp_sentence)\n",
    "        temp_sentence = tokenizer(temp_sentence)\n",
    "        #print(temp_sentence)\n",
    "        data['content'] = (temp_sentence, e1, e2)\n",
    "        \n",
    "        category_line = lines[i+1].strip()\n",
    "        if category_line != 'Other':\n",
    "            data['category'] = (category_line[:-7], category_line[-5], category_line[-2])\n",
    "        else:\n",
    "            data['category'] = (category_line,None,None)\n",
    "        \n",
    "        comment_line = lines[i+2].strip()\n",
    "        \n",
    "        datas.append(data)\n",
    "    return datas\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(datas, MAX_POS = 15, label_factorize = False):\n",
    "    sents = []\n",
    "    position_indices_1= []\n",
    "    position_indices_2= []\n",
    "    labels = []\n",
    "    for data in datas:\n",
    "        # Get content data from input data\n",
    "        sentence, e1, e2 = data['content']\n",
    "        \n",
    "        # Change entities as a original word\n",
    "        words = sentence.split()\n",
    "        e1_index = e2_index = None\n",
    "        for i, word in enumerate(words):\n",
    "            if word == 'E1':\n",
    "                words[i] = e1\n",
    "                e1_index = i\n",
    "            elif word == 'E2':\n",
    "                words[i] = e2\n",
    "                e2_index = i\n",
    "        sents.append(words)\n",
    "        #print(words)\n",
    "        \n",
    "        # Get position embedding index\n",
    "        position_index_1 = []\n",
    "        position_index_2 = []\n",
    "        for i in range(len(words)):\n",
    "            position_index_1.append(int(MAX_POS * abs(i - e1_index) / (i - e1_index)) if abs(i - e1_index) > MAX_POS else i - e1_index)\n",
    "            position_index_2.append(int(MAX_POS * abs(i - e2_index) / (i - e2_index)) if abs(i - e2_index) > MAX_POS else i - e2_index)\n",
    "            \n",
    "            position_index_1[i] += MAX_POS\n",
    "            position_index_2[i] += MAX_POS\n",
    "        position_indices_1.append(position_index_1)\n",
    "        position_indices_2.append(position_index_2)\n",
    "        #print(position_index_1)\n",
    "        #print(position_index_2)\n",
    "        \n",
    "        # Make label\n",
    "        if not label_factorize:\n",
    "            #print(data['category'])\n",
    "            label, t1, t2 = data['category']\n",
    "            if label != 'Other':\n",
    "                labels.append(label + '(e' + t1+',e'+ t2+')')\n",
    "            else:\n",
    "                labels.append(label)\n",
    "    \n",
    "    return list(zip(sents, position_indices_1, position_indices_2)), labels\n",
    "    #return {'sents': sents, 'position_indices_1': position_indices_1, 'position_indices_2': position_indices_2, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = SemEval10_task8(sub_path='SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "#train_data = preprocess_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input = train_data[0]\n",
    "#train_output = train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_x, batch_y = create_batches(train_input, train_output, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(batch_y[0]))\n",
    "#print(len(batch_y))\n",
    "#print(len(batch_x[0]))\n",
    "#print(len(batch_x))\n",
    "#print(batch_x[0])\n",
    "#print(batch_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Longtensor([1,2,3,4,5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
