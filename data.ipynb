{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import spacy\n",
    "#spacy_en = spacy.load('en')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_batch(ids, x, y):\n",
    "    batch_x = np.row_stack( [ [x[i]] for i in ids ] )\n",
    "    batch_y = np.array( [ y[i] for i in ids ] )\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "# shuffle training examples and create mini-batches\n",
    "def create_batches(x, y, batch_size, shuffle = True):\n",
    "    \n",
    "    perm = list(range(len(y)))\n",
    "    if shuffle:\n",
    "        random.shuffle(perm)\n",
    "\n",
    "    # sort sequences based on their length\n",
    "    # permutation is necessary if we want different batches every epoch\n",
    "    lst = sorted(perm, key=lambda i: len(x[i][0]))\n",
    "    #print(lst)\n",
    "    batches_x = [ ]\n",
    "    batches_y = [ ]\n",
    "    size = batch_size\n",
    "    ids = [ lst[0] ]\n",
    "    for i in lst[1:]:\n",
    "        if len(ids) < size and len(x[i][0]) == len(x[ids[0]][0]):\n",
    "            ids.append(i)\n",
    "        else:\n",
    "            bx, by = create_one_batch(ids, x, y)\n",
    "            batches_x.append(bx)\n",
    "            batches_y.append(by)\n",
    "            ids = [ i ]\n",
    "    bx, by = create_one_batch(ids, x, y)\n",
    "    batches_x.append(bx)\n",
    "    batches_y.append(by)\n",
    "\n",
    "    # shuffle batches\n",
    "    batch_perm = list(range(len(batches_x)))\n",
    "    random.shuffle(batch_perm)\n",
    "    batches_x = [ batches_x[i] for i in batch_perm ]\n",
    "    batches_y = [ batches_y[i] for i in batch_perm ]\n",
    "    return batches_x, batches_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    #return \" \".join([tok.text for tok in spacy_en.tokenizer(text)])\n",
    "    #print(text)\n",
    "    #print(word_tokenize(text))\n",
    "    return \" \".join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(datas, test_rate = 0.2):\n",
    "    random.shuffle(datas)\n",
    "    pivot_index = int(len(datas) * (1-test_rate))\n",
    "    return datas[:pivot_index], datas[pivot_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1 = '<e1>.+</e1>'\n",
    "p2 = '<e2>.+</e2>'\n",
    "\n",
    "def SemEval10_task8(sub_path, path = './dataset/SemEval2010_task8_all_data/'):\n",
    "    full_path = path + sub_path\n",
    "    \n",
    "    fin = open(full_path, 'r')\n",
    "    lines = fin.readlines()\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in range(0, len(lines), 4):\n",
    "        data = {'content': [],\n",
    "                 'category': []}\n",
    "        \n",
    "        content_line = lines[i].strip()\n",
    "        if not content_line:\n",
    "            break\n",
    "        _, raw_sentence = content_line.split('\\t')\n",
    "        raw_sentence = raw_sentence[1:-1]\n",
    "        \n",
    "        #print(raw_sentence)\n",
    "        #\n",
    "        e1_span = re.search(p1, raw_sentence).span()\n",
    "        e1 = raw_sentence[e1_span[0]+4:e1_span[1]-5]\n",
    "        \n",
    "        \n",
    "        e2_span = re.search(p2, raw_sentence).span()\n",
    "        e2 = raw_sentence[e2_span[0]+4:e2_span[1]-5]\n",
    "        \n",
    "        temp_sentence = re.sub(p1, ' E1 ', raw_sentence)\n",
    "        temp_sentence = re.sub(p2, ' E2 ', temp_sentence)\n",
    "        temp_sentence = re.sub('  ', ' ', temp_sentence)\n",
    "        temp_sentence = tokenizer(temp_sentence)\n",
    "        #print(temp_sentence)\n",
    "        data['content'] = (temp_sentence, e1, e2)\n",
    "        \n",
    "        category_line = lines[i+1].strip()\n",
    "        if category_line != 'Other':\n",
    "            data['category'] = (category_line[:-7], category_line[-5], category_line[-2])\n",
    "        else:\n",
    "            data['category'] = (category_line,None,None)\n",
    "        \n",
    "        comment_line = lines[i+2].strip()\n",
    "        \n",
    "        datas.append(data)\n",
    "    return datas\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef preprocess_dataset(datas, MAX_POS = 15, label_factorize = False, entity_normalize = False):\\n    sents = []\\n    position_indices_1= []\\n    position_indices_2= []\\n    labels = []\\n    for data in datas:\\n        # Get content data from input data\\n        sentence, e1, e2 = data['content']\\n        \\n        # Change entities as a original word\\n        words = sentence.split()\\n        e1_index = e2_index = None\\n        for i, word in enumerate(words):\\n            if word == 'E1':\\n                if not entity_normalize:\\n                    words[i] = e1\\n                e1_index = i\\n            elif word == 'E2':\\n                if not entity_normalize:\\n                    words[i] = e2\\n                e2_index = i\\n        sents.append(words)\\n        #print(words)\\n        \\n        # Get position embedding index\\n        position_index_1 = []\\n        position_index_2 = []\\n        for i in range(len(words)):\\n            position_index_1.append(int(MAX_POS * abs(i - e1_index) / (i - e1_index)) if abs(i - e1_index) > MAX_POS else i - e1_index)\\n            position_index_2.append(int(MAX_POS * abs(i - e2_index) / (i - e2_index)) if abs(i - e2_index) > MAX_POS else i - e2_index)\\n            \\n            position_index_1[i] += MAX_POS\\n            position_index_2[i] += MAX_POS\\n        position_indices_1.append(position_index_1)\\n        position_indices_2.append(position_index_2)\\n        #print(position_index_1)\\n        #print(position_index_2)\\n        \\n        # Make label\\n        if not label_factorize:\\n            #print(data['category'])\\n            label, t1, t2 = data['category']\\n            if label != 'Other':\\n                labels.append(label + '(e' + t1+',e'+ t2+')')\\n            else:\\n                labels.append(label)\\n    \\n    return list(zip(sents, position_indices_1, position_indices_2)), labels\\n    #return {'sents': sents, 'position_indices_1': position_indices_1, 'position_indices_2': position_indices_2, 'labels': labels}\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess_dataset(datas, MAX_POS = 15, label_factorize = False, entity_normalize = False):\n",
    "    sents = []\n",
    "    position_indices_1= []\n",
    "    position_indices_2= []\n",
    "    labels = []\n",
    "    for data in datas:\n",
    "        # Get content data from input data\n",
    "        sentence, e1, e2 = data['content']\n",
    "        \n",
    "        # Change entities as a original word\n",
    "        words = sentence.split()\n",
    "        e1_index = e2_index = None\n",
    "        for i, word in enumerate(words):\n",
    "            if word == 'E1':\n",
    "                if not entity_normalize:\n",
    "                    words[i] = e1\n",
    "                e1_index = i\n",
    "            elif word == 'E2':\n",
    "                if not entity_normalize:\n",
    "                    words[i] = e2\n",
    "                e2_index = i\n",
    "        sents.append(words)\n",
    "        #print(words)\n",
    "        \n",
    "        # Get position embedding index\n",
    "        position_index_1 = []\n",
    "        position_index_2 = []\n",
    "        for i in range(len(words)):\n",
    "            position_index_1.append(int(MAX_POS * abs(i - e1_index) / (i - e1_index)) if abs(i - e1_index) > MAX_POS else i - e1_index)\n",
    "            position_index_2.append(int(MAX_POS * abs(i - e2_index) / (i - e2_index)) if abs(i - e2_index) > MAX_POS else i - e2_index)\n",
    "            \n",
    "            position_index_1[i] += MAX_POS\n",
    "            position_index_2[i] += MAX_POS\n",
    "        position_indices_1.append(position_index_1)\n",
    "        position_indices_2.append(position_index_2)\n",
    "        #print(position_index_1)\n",
    "        #print(position_index_2)\n",
    "        \n",
    "        # Make label\n",
    "        if not label_factorize:\n",
    "            #print(data['category'])\n",
    "            label, t1, t2 = data['category']\n",
    "            if label != 'Other':\n",
    "                labels.append(label + '(e' + t1+',e'+ t2+')')\n",
    "            else:\n",
    "                labels.append(label)\n",
    "    \n",
    "    return list(zip(sents, position_indices_1, position_indices_2)), labels\n",
    "    #return {'sents': sents, 'position_indices_1': position_indices_1, 'position_indices_2': position_indices_2, 'labels': labels}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(datas, MAX_POS = 15, label_factorize = False, entity_normalize = False, directional_consideration = True):\n",
    "    sents = []\n",
    "    position_indices_1= []\n",
    "    position_indices_2= []\n",
    "    labels = []\n",
    "    for data in datas:\n",
    "        # Get content data from input data\n",
    "        sentence, e1, e2 = data['content']\n",
    "        \n",
    "        # Change entities as a original word\n",
    "        words = sentence.split()\n",
    "        e1_index = e2_index = None\n",
    "        e1_len = e2_len = None\n",
    "        input_words = []\n",
    "        \n",
    "        index = 0\n",
    "        for word in words:\n",
    "            #print(input_words)\n",
    "            if word == 'E1':\n",
    "                if entity_normalize:\n",
    "                    input_words.append('e1')\n",
    "                    index += 1\n",
    "                else:\n",
    "                    \n",
    "                    tokens = e1.strip().split(' ')\n",
    "                    \n",
    "                    input_words += ['<e1>']\n",
    "                    for token in tokens:\n",
    "                        \n",
    "                        input_words += [token]\n",
    "                    input_words += ['</e1>']\n",
    "                    e1_index = index\n",
    "                    e1_len = len(tokens)+2\n",
    "                    index += e1_len\n",
    "            elif word == 'E2':\n",
    "                if entity_normalize:\n",
    "                    input_words.append('e2')\n",
    "                    index += 1\n",
    "                else:\n",
    "                    \n",
    "                    tokens = e2.strip().split(' ')\n",
    "                    \n",
    "                    input_words += ['<e2>']\n",
    "                    for token in tokens:\n",
    "                        input_words += [token]\n",
    "                    input_words += ['</e2>']\n",
    "                    e2_index = index\n",
    "                    e2_len = len(tokens)+2\n",
    "                    index += e2_len\n",
    "            else:\n",
    "                input_words += [word]\n",
    "                index += 1\n",
    "        sents.append(input_words)\n",
    "        #print(input_words)\n",
    "        #print(words)\n",
    "        \n",
    "        # Get position embedding index\n",
    "        position_index_1 = []\n",
    "        position_index_2 = []\n",
    "        for i in range(len(input_words)):\n",
    "            \n",
    "            if i < e1_index:\n",
    "                pos1 =   i - e1_index\n",
    "            elif i >= e1_index + e1_len:\n",
    "                pos1 = i - (e1_index + e1_len - 1)\n",
    "            else:\n",
    "                pos1 = 0\n",
    "            \n",
    "            \n",
    "            if i < e2_index:\n",
    "                pos2 =   i - e2_index\n",
    "            elif i >= e2_index + e2_len:\n",
    "                pos2 = i - (e2_index + e2_len - 1)\n",
    "            else:\n",
    "                pos2 = 0\n",
    "            #print(pos2)\n",
    "            position_index_1.append(int(MAX_POS * abs(pos1) / (pos1)) if abs(pos1) > MAX_POS else pos1)\n",
    "            position_index_2.append(int(MAX_POS * abs(pos2) / (pos2)) if abs(pos2) > MAX_POS else pos2)\n",
    "            #position_index_2.append(pos2 + MAX_POS)\n",
    "            #position_index_1.append(pos1 + MAX_POS)\n",
    "            #print(input_words[e1_index:e1_index+e1_len])\n",
    "            #print(input_words[e2_index:e2_index+e2_len])\n",
    "            \n",
    "            \n",
    "            position_index_1[i] += MAX_POS\n",
    "            position_index_2[i] += MAX_POS\n",
    "        position_indices_1.append(position_index_1)\n",
    "        position_indices_2.append(position_index_2)\n",
    "        #print(position_index_1)\n",
    "        #print(position_index_2)\n",
    "        \n",
    "        # Make label\n",
    "        if not label_factorize:\n",
    "            #print(data['category'])\n",
    "            label, t1, t2 = data['category']\n",
    "            if label != 'Other':\n",
    "                if directional_consideration:\n",
    "                    labels.append(label + '(e' + t1+',e'+ t2+')')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            else:\n",
    "                labels.append(label)\n",
    "    \n",
    "    return list(zip(sents, position_indices_1, position_indices_2)), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = SemEval10_task8(sub_path='SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "#train_data = preprocess_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input = train_data[0]\n",
    "#train_output = train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_x, batch_y = create_batches(train_input, train_output, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(batch_y[0]))\n",
    "#print(len(batch_y))\n",
    "#print(len(batch_x[0]))\n",
    "#print(len(batch_x))\n",
    "#print(batch_x[0])\n",
    "#print(batch_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.Longtensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Variables or dicts, but got torch.autograd.variable.Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-acff7fd96328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay)\u001b[0m\n\u001b[1;32m     27\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     28\u001b[0m                         weight_decay=weight_decay)\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     25\u001b[0m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[1;32m     26\u001b[0m                             \u001b[0;34m\"an iterable of Variables or dicts, but got \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                             torch.typename(params))\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Variables or dicts, but got torch.autograd.variable.Variable"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "a = Variable(torch.rand(100))\n",
    "optimizer = optim.Adam(a, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
