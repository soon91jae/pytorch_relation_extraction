{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from visdom import Visdom\n",
    "viz = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import tokenizer, data_split, preprocess_dataset, create_batches\n",
    "from data import SemEval10_task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Baseline_Model(nn.Module):\n",
    "    def __init__(self, word_vocab, label_vocab, word_emb_dim, pos_emb_dim, hidden_dim, output_dim, MAX_POS = 15, use_gpu = True):\n",
    "        super(LSTM_Baseline_Model, self).__init__()\n",
    "        \n",
    "        # Set hyper parameters\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.pos_emb_dim = pos_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = word_emb_dim + pos_emb_dim * 2\n",
    "        \n",
    "        self.MAX_POS = MAX_POS\n",
    "        \n",
    "        \n",
    "        # Set options and other parameters\n",
    "        self.use_gpu = use_gpu\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        #self.pos_vocab = pos_vocab\n",
    "        \n",
    "        \n",
    "        # Free parameters for the model\n",
    "        # Initialize embeddings (Word and Position embeddings) \n",
    "        self.word_emb = nn.Embedding(len(self.word_vocab), self.word_emb_dim).cuda()\n",
    "        \n",
    "        self.pos1_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos1_emb.weight.data.uniform_(-0.04, 0.04)\n",
    "        self.pos2_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos2_emb.weight.data.uniform_(-0.04, 0.04)\n",
    "        \n",
    "        # Initialize LSTM parameters ()\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, bidirectional=True).cuda()\n",
    "        \n",
    "        \n",
    "        # Initialize Attention parameters ()\n",
    "        self.attention_hidden = nn.Linear(hidden_dim * 2, hidden_dim).cuda()\n",
    "        self.attention = nn.Linear(hidden_dim, 1).cuda()\n",
    "        \n",
    "        # Initialize Classifier parameters ()\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim).cuda()\n",
    "        \n",
    "        \n",
    "        self.word_emb.weight.data.copy_(word_vocab.vectors)\n",
    "    def prepare_inout(self, X, y):\n",
    "        sents, pos1, pos2 = list(zip(*X))\n",
    "        #sents = list(zip(*X))\n",
    "        #pos1 = datas['position_indices_1']\n",
    "        #pos2 = datas['position_indices_2']\n",
    "        \n",
    "        labels = y\n",
    "        \n",
    "        words = [ [ self.word_vocab.stoi[word] for word in sent] for sent in sents]\n",
    "        #print(words)\n",
    "        words_var = Variable(torch.LongTensor(words).cuda())\n",
    "        #print(words_var)\n",
    "        word_embeddings = self.word_emb(words_var)\n",
    "        #print(word_embedings)\n",
    "        \n",
    "        pos1 = np.array(pos1).astype('int')\n",
    "        #print(pos1)\n",
    "        pos1_var = Variable(torch.LongTensor(pos1).cuda())\n",
    "        pos1_embeddings = self.pos1_emb(pos1_var)\n",
    "        #print(pos1_var)\n",
    "        #print(pos1_embeddings)\n",
    "\n",
    "        pos2 = np.array(pos2).astype('int')\n",
    "        #print(pos2)\n",
    "        pos2_var = Variable(torch.LongTensor(pos2).cuda())\n",
    "        pos2_embeddings = self.pos2_emb(pos2_var)\n",
    "        \n",
    "        inputs = torch.cat((word_embeddings, pos1_embeddings, pos2_embeddings),-1)\n",
    "        #print(inputs)\n",
    "        \n",
    "        labels = [ self.label_vocab.stoi[label] - 1 for label in labels]\n",
    "        labels_var = Variable(torch.LongTensor(labels).cuda())\n",
    "        outputs = labels_var\n",
    "        \n",
    "        return inputs, outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, X, is_train = True):\n",
    "        # LSTM layer\n",
    "        X = F.dropout(X, p=0.3, training=is_train)\n",
    "        hiddens, for_output = self.lstm(X)\n",
    "        #rev_hiddens, rev_output = self.rev_lstm(X)\n",
    "        hiddens = F.dropout(hiddens, p=0.3, training=is_train)\n",
    "        \n",
    "        # Self Attentive layer\n",
    "        att_hidden = F.tanh(self.attention_hidden(hiddens))\n",
    "        \n",
    "        att_scores = self.attention(att_hidden)\n",
    "        \n",
    "        attention_distrib = F.softmax(att_scores, dim = 1)\n",
    "        context_vector = F.tanh(torch.sum(hiddens * attention_distrib, dim = 1))\n",
    "\n",
    "        # Classifier\n",
    "        context_vector = F.dropout(context_vector, p=0.5, training=is_train)\n",
    "        finals = F.softmax(self.classifier(context_vector), dim = 1)\n",
    "\n",
    "        return finals\n",
    "    \n",
    "    def evaluatation(self, input, output, demonstrate_result = True, analyze = False, header=\"\"):\n",
    "        batch_Xs, batch_ys = create_batches(input, output, 128, shuffle=False)\n",
    "        #loss = 0\n",
    "        tp = 0\n",
    "        for batch_X, batch_Y in zip(batch_Xs, batch_ys):\n",
    "            X, Y = self.prepare_inout(batch_X, batch_Y)\n",
    "            preds = relation_extr(X, is_train = False)\n",
    "            _, preds_Y = torch.max(preds, -1)\n",
    "            tp += (preds_Y == Y).float().sum().data.cpu().numpy()[0]\n",
    "            del X,Y\n",
    "            \n",
    "        if demonstrate_result:\n",
    "            #print('Avg loss: ')\n",
    "            print(header + \" accuracy: \", tp/float(len(output)))\n",
    "            \n",
    "            #print('Macro F1-score')\n",
    "            #print('Micro F1-score')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = data_split(SemEval10_task8(sub_path='SemEval2010_task8_training/TRAIN_FILE.TXT'), test_rate = 0.1)\n",
    "test = SemEval10_task8(sub_path='SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "\n",
    "train_input, train_output = preprocess_dataset(train)\n",
    "train_words = list(zip(*train_input))[0]\n",
    "\n",
    "dev_input, dev_output = preprocess_dataset(dev)\n",
    "dev_words = list(zip(*dev_input))[0]\n",
    "\n",
    "test_input, test_output = preprocess_dataset(test)\n",
    "test_words = list(zip(*test_input))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  lower=False)\n",
    "TEXT.build_vocab(train_words+test_words+dev_words, vectors=\"glove.840B.300d\")\n",
    "word_vocab = TEXT.vocab\n",
    "\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "LABEL.build_vocab(train_output+test_output+dev_output)\n",
    "label_vocab = LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "#l2_rate = 10e-4\n",
    "\n",
    "max_batch_size = 1\n",
    "#max_num_of_sent = 50\n",
    "word_emb_dim = 300\n",
    "pos_emb_dim = 5\n",
    "hidden_dim = 230\n",
    "\n",
    "print(len(LABEL.vocab.stoi))\n",
    "relation_extr = LSTM_Baseline_Model(word_vocab, \n",
    "                                    label_vocab, \n",
    "                                    word_emb_dim = word_emb_dim, \n",
    "                                    pos_emb_dim = pos_emb_dim, \n",
    "                                    hidden_dim = hidden_dim, \n",
    "                                    output_dim = len(LABEL.vocab.stoi)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(relation_extr.parameters())\n",
    "#print(params)\n",
    "# gradient clip\n",
    "torch.nn.utils.clip_grad_norm(params, 5.0)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params, lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0 2.7864286074704596\n",
      "Train accuracy 0 0.2461111111111111\n",
      "Dev accuracy:  0.27375\n",
      "Test accuracy:  0.2686786897313213\n",
      "\n",
      "Train epoch 1 2.7052458430661095\n",
      "Train accuracy 1 0.32666666666666666\n",
      "Dev accuracy:  0.345\n",
      "Test accuracy:  0.31762973868237027\n",
      "\n",
      "Train epoch 2 2.6094179738230174\n",
      "Train accuracy 2 0.42194444444444446\n",
      "Dev accuracy:  0.4325\n",
      "Test accuracy:  0.40743467059256533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epoch_num):\n",
    "    epoch_loss = 0\n",
    "    tp = 0\n",
    "    batch_Xs, batch_ys = create_batches(train_input, train_output, max_batch_size)\n",
    "    for batch_X, batch_y in zip(batch_Xs, batch_ys):\n",
    "        X, Y = relation_extr.prepare_inout(batch_X, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = relation_extr(X)\n",
    "        #print(preds)\n",
    "        _, preds_Y = torch.max(preds, -1)\n",
    "        #print(preds_Y)\n",
    "        batch_size = Y.size()[0]\n",
    "        loss = loss_func(preds, Y)\n",
    "        batch_loss = loss\n",
    "        #batch_loss = loss\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu().numpy()[0]\n",
    "        tp += (preds_Y == Y).float().sum().data.cpu().numpy()[0]\n",
    "        del X, Y, loss, batch_loss, preds, preds_Y\n",
    "    print(\"Train epoch\",i,epoch_loss/len(train_output))\n",
    "    print(\"Train accuracy\",i,tp/float(len(train_output)))\n",
    "    relation_extr.evaluatation(dev_input, dev_output,header = 'Dev')\n",
    "    relation_extr.evaluatation(test_input, test_output,header = 'Test')\n",
    "    #print(params[0])\n",
    "    \n",
    "    del epoch_loss, tp\n",
    "    \n",
    "    \n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.rand(64,10,320).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Baseline(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0]*10]*20\n",
    "print(a)\n",
    "a[0][0] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
