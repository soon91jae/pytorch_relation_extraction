{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from visdom import Visdom\n",
    "viz = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import tokenizer, data_split, preprocess_dataset, create_batches\n",
    "from data import SemEval10_task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Baseline_Model(nn.Module):\n",
    "    def __init__(self, word_vocab, label_vocab, word_emb_dim, pos_emb_dim, hidden_dim, output_dim, MAX_POS = 15, use_gpu = True):\n",
    "        super(LSTM_Baseline_Model, self).__init__()\n",
    "        \n",
    "        # Set hyper parameters\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.pos_emb_dim = pos_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = word_emb_dim + pos_emb_dim * 2\n",
    "        \n",
    "        self.MAX_POS = MAX_POS\n",
    "        \n",
    "        \n",
    "        # Set options and other parameters\n",
    "        self.use_gpu = use_gpu\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        #self.pos_vocab = pos_vocab\n",
    "        \n",
    "        \n",
    "        # Free parameters for the model\n",
    "        # Initialize embeddings (Word and Position embeddings) \n",
    "        self.word_emb = nn.Embedding(len(self.word_vocab), self.word_emb_dim).cuda()\n",
    "        \n",
    "        self.pos1_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos1_emb.weight.data.uniform_(-0.04, 0.04)\n",
    "        self.pos2_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos2_emb.weight.data.uniform_(-0.04, 0.04)\n",
    "        \n",
    "        # Initialize LSTM parameters ()\n",
    "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, bidirectional=True, batch_first = True).cuda()\n",
    "        \n",
    "        \n",
    "        # Initialize Attention parameters ()\n",
    "        self.attention_hidden = nn.Linear(hidden_dim * 2, hidden_dim,bias=False).cuda()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False).cuda()\n",
    "        \n",
    "        # Initialize Classifier parameters ()\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim).cuda()\n",
    "        \n",
    "        \n",
    "        self.word_emb.weight.data.copy_(word_vocab.vectors)\n",
    "    def prepare_inout(self, X, y):\n",
    "        sents, pos1, pos2 = list(zip(*X))\n",
    "        #sents = list(zip(*X))\n",
    "        #pos1 = datas['position_indices_1']\n",
    "        #pos2 = datas['position_indices_2']\n",
    "        \n",
    "        labels = y\n",
    "        \n",
    "        words = [ [ self.word_vocab.stoi[word] for word in sent] for sent in sents]\n",
    "        #print(words)\n",
    "        words_var = Variable(torch.LongTensor(words).cuda())\n",
    "        #print(words_var)\n",
    "        word_embeddings = self.word_emb(words_var)\n",
    "        #print(word_embedings)\n",
    "        \n",
    "        pos1 = np.array(pos1).astype('int')\n",
    "        #print(pos1)\n",
    "        pos1_var = Variable(torch.LongTensor(pos1).cuda())\n",
    "        pos1_embeddings = self.pos1_emb(pos1_var)\n",
    "        #print(pos1_var)\n",
    "        #print(pos1_embeddings)\n",
    "\n",
    "        pos2 = np.array(pos2).astype('int')\n",
    "        #print(pos2)\n",
    "        pos2_var = Variable(torch.LongTensor(pos2).cuda())\n",
    "        pos2_embeddings = self.pos2_emb(pos2_var)\n",
    "        \n",
    "        inputs = torch.cat((word_embeddings, pos1_embeddings, pos2_embeddings),-1)\n",
    "        #print(inputs)\n",
    "        \n",
    "        labels = [ self.label_vocab.stoi[label] - 1 for label in labels]\n",
    "        labels_var = Variable(torch.LongTensor(labels).cuda())\n",
    "        outputs = labels_var\n",
    "        \n",
    "        return inputs, outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, X, is_train = True):\n",
    "        # LSTM layer\n",
    "        X = F.dropout(X, p=0.3, training=is_train)\n",
    "        hiddens, for_output = self.lstm(X)\n",
    "        #rev_hiddens, rev_output = self.rev_lstm(X)\n",
    "        hiddens = F.dropout(hiddens, p=0.5, training=is_train)\n",
    "        \n",
    "        # Self Attentive layer\n",
    "        att_hidden = F.tanh(self.attention_hidden(hiddens))\n",
    "        \n",
    "        att_scores = self.attention(att_hidden)\n",
    "        \n",
    "        attention_distrib = F.softmax(att_scores, dim = 1)\n",
    "        context_vector = torch.sum(hiddens * attention_distrib, dim = 1)\n",
    "\n",
    "        # Classifier\n",
    "        context_vector = F.dropout(context_vector, p=0.5, training=is_train)\n",
    "        finals = F.softmax(self.classifier(context_vector), dim = 1)\n",
    "\n",
    "        return finals\n",
    "    \n",
    "    def evaluatation(self, input, output, demonstrate_result = True, analyze = False, header=\"\"):\n",
    "        batch_Xs, batch_ys = create_batches(input, output, 128, shuffle=False)\n",
    "        #loss = 0\n",
    "        tp = 0\n",
    "        for batch_X, batch_Y in zip(batch_Xs, batch_ys):\n",
    "            X, Y = self.prepare_inout(batch_X, batch_Y)\n",
    "            preds = relation_extr(X, is_train = False)\n",
    "            _, preds_Y = torch.max(preds, -1)\n",
    "            tp += (preds_Y == Y).float().sum().data.cpu().numpy()[0]\n",
    "            del X,Y\n",
    "            \n",
    "        if demonstrate_result:\n",
    "            #print('Avg loss: ')\n",
    "            print(header + \" accuracy: \", tp/float(len(output)))\n",
    "            \n",
    "            #print('Macro F1-score')\n",
    "            #print('Micro F1-score')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = data_split(SemEval10_task8(sub_path='SemEval2010_task8_training/TRAIN_FILE.TXT'), test_rate = 0.1)\n",
    "test = SemEval10_task8(sub_path='SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "\n",
    "train_input, train_output = preprocess_dataset(train)\n",
    "train_words = list(zip(*train_input))[0]\n",
    "\n",
    "dev_input, dev_output = preprocess_dataset(dev)\n",
    "dev_words = list(zip(*dev_input))[0]\n",
    "\n",
    "test_input, test_output = preprocess_dataset(test)\n",
    "test_words = list(zip(*test_input))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  lower=False)\n",
    "TEXT.build_vocab(train_words+test_words+dev_words, vectors=\"glove.840B.300d\")\n",
    "word_vocab = TEXT.vocab\n",
    "\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "LABEL.build_vocab(train_output+test_output+dev_output)\n",
    "label_vocab = LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 200\n",
    "\n",
    "learning_rate = 0.0003\n",
    "#l2_rate = 10e-4\n",
    "\n",
    "max_batch_size = 16\n",
    "#max_num_of_sent = 50\n",
    "word_emb_dim = 300\n",
    "pos_emb_dim = 15\n",
    "hidden_dim = 320\n",
    "\n",
    "print(len(LABEL.vocab.stoi))\n",
    "relation_extr = LSTM_Baseline_Model(word_vocab, \n",
    "                                    label_vocab, \n",
    "                                    word_emb_dim = word_emb_dim, \n",
    "                                    pos_emb_dim = pos_emb_dim, \n",
    "                                    hidden_dim = hidden_dim, \n",
    "                                    output_dim = len(LABEL.vocab.stoi)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('word_emb.weight', Parameter containing:\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.2720 -0.0620 -0.1884  ...   0.1302 -0.1832  0.1323\n",
      "          ...             ⋱             ...          \n",
      " 0.0070 -0.0086 -0.6045  ...   0.6283  0.4505  0.0291\n",
      "-0.4984  0.5471 -0.1409  ...   0.1861 -0.0487  0.4646\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 26399x300 (GPU 0)]\n",
      "), ('pos1_emb.weight', Parameter containing:\n",
      "1.00000e-02 *\n",
      " -1.5874  1.7058 -0.3536 -2.8543 -2.0106 -3.2879  2.7891  3.1329  2.1524 -1.6334\n",
      "  3.9104  0.4944 -0.3396  0.2564 -0.4568  0.9153 -1.7000  2.5671  1.7345  0.2495\n",
      "  3.1816 -0.6301  2.8543 -2.8053  2.5533  2.9146  0.1264  3.1328 -3.4234 -3.6173\n",
      " -0.8508 -1.3331  0.8847 -0.6192 -1.4569 -0.8130  0.8859 -2.6316  1.7681 -1.3402\n",
      " -1.2913  2.2774 -1.1795 -2.6168  0.0351  3.6502 -0.6231 -2.1177  0.8026 -1.5547\n",
      "  1.4115 -1.8352 -3.4587 -2.7188 -2.0758  2.7975  1.3593  0.5849 -0.9611 -2.9419\n",
      "  0.3509 -1.5143 -1.7243  0.9742  2.2764  3.4736 -0.9222 -3.0692  1.1777  3.7469\n",
      " -1.0161  0.1018  1.9325  2.3862  1.9212  3.3993 -3.7921  2.0594 -1.0625  3.8069\n",
      "  2.3121  2.9938 -1.4857  0.2566  0.8801 -0.7941  0.5359  3.2349 -3.8022  1.7764\n",
      " -0.1211  2.6963 -2.0643 -0.7808 -1.3716 -0.9999  3.6564 -0.7919 -1.8859  1.7168\n",
      "  0.0630 -3.1186 -2.8551  1.7703 -0.4449 -1.4948  2.1352  0.8989 -3.8867  1.3382\n",
      "  3.7700 -0.8497 -0.6816 -0.6367 -3.9529  2.8690  1.6955  1.7367  0.7719 -1.9978\n",
      "  1.4425 -2.3407  2.1325 -2.5979  3.9739  0.7565 -1.0300  0.0129  1.9255 -0.9845\n",
      " -1.3847  1.8209 -3.3497 -0.8554 -3.3227  2.5550  2.3200 -1.7471 -1.5482  2.1477\n",
      "  2.6299  1.5901  1.9798 -1.7745 -1.8987 -1.6213  0.9778  0.2060 -3.7710 -1.3068\n",
      " -2.4477  2.5707 -0.6039  0.3528  2.8130 -1.4562 -2.9653 -2.6724  1.4738  2.5451\n",
      " -1.7122  0.2324 -0.7304  2.4569 -0.3544 -3.4518  0.7773 -2.3051  0.6349  1.5384\n",
      "  3.8290  0.3939 -3.2194 -0.6725  3.4493  3.2834 -0.9618  2.5448 -2.6740  1.6830\n",
      " -2.7105  0.8877  0.1884 -0.2566 -1.7857 -1.2616 -2.1273  3.7420  0.5421  0.1150\n",
      "  1.2982 -1.0709  3.6014 -1.1613 -2.4737 -3.0470 -1.6563 -0.1488 -0.6124 -0.7611\n",
      "  3.4466 -0.1514  3.7908 -1.8468  1.2637  0.0491  1.3521 -2.0618 -3.2547  1.4914\n",
      " -0.0479  1.5013 -0.4041 -3.9425 -0.3311  3.7448 -1.9995 -1.9122 -3.9197  3.6870\n",
      " -3.4962  0.9935 -3.8231  2.7305 -1.7856 -2.4334 -1.2732  0.9852 -1.2445  0.8485\n",
      " -3.2782  3.8891  2.8328 -3.0443 -2.2123 -1.2140 -2.0103 -0.2872 -1.4946 -1.3786\n",
      " -1.3323 -0.6040 -0.6682 -0.9442  3.8555  1.1155  0.0143 -2.4598 -2.9185  0.2490\n",
      "  2.1487 -3.2870 -2.2141  3.0214 -0.3493  0.0102 -2.2664  3.8016 -0.3659  1.6370\n",
      "  2.7207  1.3576  3.1139 -1.7245  2.6919  0.5974 -2.9067  2.7037 -0.7658  2.9243\n",
      "  2.1342 -1.6900 -0.1013  0.0545  0.8716 -2.9405 -2.1918  1.3954  3.1373 -2.0496\n",
      " -3.9643  0.1696 -3.9957 -1.1202  0.3215 -0.6980 -2.4585  0.7714  2.3823 -1.8474\n",
      " -1.0565 -3.6098  2.1535  2.5694  0.3369 -3.0976 -3.4471  2.4624 -2.5098  0.3381\n",
      "  2.2324  3.3754 -0.8787  0.6172  3.1857  2.3412 -2.1546 -3.1357  3.0385 -0.2860\n",
      "[torch.cuda.FloatTensor of size 31x10 (GPU 0)]\n",
      "), ('pos2_emb.weight', Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.4430 -3.6557  3.6137 -1.6855  0.7063 -0.1076 -1.1392  2.3803 -0.1061  2.9493\n",
      "  2.5415  2.0996  2.3493  1.6004 -0.4716 -1.6690  3.6664 -0.9318 -2.8737 -3.5369\n",
      "  2.7158 -0.6477 -0.5889  1.2368 -2.7339  1.2400 -3.6654  0.6920 -3.1498 -3.8885\n",
      " -3.2260  1.8773 -2.6254  0.9044  3.5062  3.0695 -1.6911 -1.4977  0.7421 -3.6814\n",
      " -0.2556 -3.7830 -3.4739 -0.0372  2.5876 -1.9348 -3.6115 -0.6628 -1.2107 -2.6020\n",
      " -1.3305  3.2841  0.1137 -0.9478 -1.6110 -0.8689 -0.7613 -2.4756  3.8930 -2.7660\n",
      " -1.9331  0.2420 -3.8858  1.0394 -2.8014  1.8855  3.9287  2.8140  2.2941 -3.5839\n",
      "  0.8159  1.4690  0.7687  0.9728 -3.0111 -2.8714 -2.4068 -2.5025  3.8361  0.9665\n",
      " -0.7126 -3.0865  3.4100  0.9059  0.9028 -1.2091  3.5293  2.9264 -3.9288  2.8189\n",
      "  3.8518  1.5102 -3.4212  3.8283  2.2395  2.4419 -3.8929  0.5223 -3.5218  1.0603\n",
      " -1.6667  1.4709  0.1325 -2.2038 -3.9917  0.9484 -2.3900  3.4016 -1.8858 -0.1845\n",
      "  1.5605  0.3305  0.1430  1.1086  3.2422  1.7067 -2.4717 -0.9660 -0.3533 -0.6901\n",
      "  3.3583  2.9103  1.1201 -2.1339 -0.6480  1.2322 -0.7154  2.8123  3.4326 -1.7444\n",
      " -2.3286  2.5321  0.8100 -1.4736  3.8858  2.4672  2.6116  0.5750  0.4072  0.1365\n",
      "  2.2732 -3.7989 -2.3301 -0.6475  3.4419  2.3442 -1.3366  3.5164 -2.2610  2.6474\n",
      " -0.0471  2.4960  3.4520 -1.8509  1.9641 -2.1851  2.6135 -3.5793 -3.1772  1.7505\n",
      " -3.4362 -3.6109  3.0024 -2.3412 -1.8191  2.0176 -3.7927 -3.2826  1.9527  1.7311\n",
      "  1.0442  2.4850 -0.0848  1.2328 -1.7060  2.5890 -2.9667  3.7585  2.2797 -0.7245\n",
      "  1.1571  2.6837 -1.1061 -2.7982 -1.3547  3.6032 -0.1480  2.3064 -0.6696 -1.0073\n",
      "  0.9327 -1.0888  2.3620 -3.2469 -0.3731 -0.9114 -0.6235  1.3863 -3.4881 -3.2938\n",
      " -1.5553  0.4757 -3.3796 -0.6645  1.5362 -1.9799 -2.0429  3.3761 -2.5325  3.0309\n",
      " -3.3413 -1.2416 -0.6157  0.5153  0.4700 -3.2709 -2.4253  1.1789 -2.3791 -1.2676\n",
      " -0.3991 -2.9096 -1.8498 -1.7700  1.3455 -0.5045  0.7949  2.6489  3.2753  3.8296\n",
      "  3.5504 -2.3275 -3.1339 -1.3204  2.6913  0.4126 -0.4633  1.7594 -1.3268 -1.0478\n",
      "  0.3433 -0.7518 -3.8799 -1.4410 -1.5499 -1.8180 -3.4274  1.5412  1.2446  2.1659\n",
      " -2.7891 -1.1660  2.5292 -2.2073  1.3810  2.3480 -3.4299  2.8004  1.6935 -0.7283\n",
      " -2.7728 -0.3123 -2.5239  3.1209 -2.1484 -3.8853  0.9128  1.5414  2.0291 -3.9416\n",
      "  1.0539 -2.3770  3.2961  2.6766  2.2381  0.9266  1.9623  2.1144 -2.5683 -1.7653\n",
      "  2.0337 -0.4232 -3.1497  1.6098  3.1956 -1.1518  3.2750  1.7693  1.8928  2.8879\n",
      "  2.2732  2.2649 -2.5967 -1.0896  1.3669  0.0534 -1.7098  1.0566 -1.8189  2.7573\n",
      " -1.2433 -2.1385  3.9800 -1.7091  1.0000  2.8642  3.3706 -0.1817 -2.5443  0.8413\n",
      "[torch.cuda.FloatTensor of size 31x10 (GPU 0)]\n",
      "), ('lstm.weight_ih_l0', Parameter containing:\n",
      "-1.7330e-02  2.3384e-02  9.1629e-03  ...   1.1126e-02  4.4936e-02  3.4479e-02\n",
      " 5.3446e-02  2.4462e-02 -5.4848e-03  ...  -1.1227e-02  2.9805e-03  3.7143e-02\n",
      "-5.3507e-02  3.8483e-02 -4.4109e-02  ...   4.8723e-03 -5.1688e-02 -6.7955e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 4.1535e-02 -4.4255e-02  1.0748e-02  ...   3.1200e-02 -3.6914e-02  5.8309e-04\n",
      " 6.6808e-03 -1.5384e-02 -3.7340e-02  ...  -4.5602e-03 -5.4740e-02  5.3247e-02\n",
      " 8.9201e-04  3.2123e-02  2.5426e-02  ...   1.0723e-02 -3.6701e-02 -2.6671e-02\n",
      "[torch.cuda.FloatTensor of size 1280x320 (GPU 0)]\n",
      "), ('lstm.weight_hh_l0', Parameter containing:\n",
      "-4.6318e-02 -2.8854e-02  4.5303e-04  ...  -5.9766e-04  4.0579e-02  3.3766e-02\n",
      " 3.8704e-02  2.9102e-02  3.9598e-02  ...   4.1139e-02  5.4128e-03  3.6594e-02\n",
      " 2.9140e-02 -3.4254e-02  2.0369e-02  ...  -2.7466e-02  1.8529e-02  2.8027e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-2.4105e-02  1.5629e-02 -9.6846e-03  ...  -5.0872e-03  4.1295e-02  3.3933e-02\n",
      " 9.0019e-03 -9.9979e-03 -1.0854e-02  ...   4.9938e-02 -3.7000e-02  4.0632e-02\n",
      " 2.9437e-02 -3.2241e-02  1.4205e-02  ...  -4.6239e-02 -1.8578e-03 -1.5431e-02\n",
      "[torch.cuda.FloatTensor of size 1280x320 (GPU 0)]\n",
      "), ('lstm.bias_ih_l0', Parameter containing:\n",
      "1.00000e-02 *\n",
      "  4.4939\n",
      "  3.2310\n",
      "  1.9448\n",
      "    ⋮   \n",
      " -4.3263\n",
      " -3.3133\n",
      " -5.4080\n",
      "[torch.cuda.FloatTensor of size 1280 (GPU 0)]\n",
      "), ('lstm.bias_hh_l0', Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.6176\n",
      " -1.1095\n",
      "  1.8111\n",
      "    ⋮   \n",
      "  5.4395\n",
      " -1.3977\n",
      " -3.7096\n",
      "[torch.cuda.FloatTensor of size 1280 (GPU 0)]\n",
      "), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      " 3.6915e-02  2.6535e-03  1.4869e-02  ...  -2.3382e-02 -1.7829e-02 -2.7522e-02\n",
      "-1.3499e-02 -3.1225e-03 -4.5662e-02  ...   4.6714e-02  4.2050e-02  3.7542e-02\n",
      " 1.3142e-02  3.6654e-02 -1.5363e-02  ...   7.1090e-03 -2.1581e-02  3.9897e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 7.7830e-03 -7.7231e-03 -2.5720e-02  ...  -3.2474e-03 -3.3685e-02 -5.0821e-02\n",
      " 5.2893e-02  4.2618e-02 -3.5373e-02  ...   4.7460e-02  3.2054e-02 -2.5913e-02\n",
      "-2.3583e-02  5.0112e-02  2.7862e-02  ...   1.9315e-02 -3.7605e-02  1.1197e-02\n",
      "[torch.cuda.FloatTensor of size 1280x320 (GPU 0)]\n",
      "), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      " 2.4439e-02  1.4374e-02  1.0514e-02  ...  -1.7332e-02  2.6557e-03  8.7141e-03\n",
      " 4.2393e-02 -3.9797e-02  1.1208e-02  ...   4.8854e-02 -3.9691e-02 -2.2902e-02\n",
      " 4.1169e-03 -3.0905e-02 -1.1299e-02  ...  -3.4951e-02 -1.0426e-02  1.8153e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 7.4245e-03 -1.9182e-02 -4.1659e-02  ...   3.3396e-02 -2.2742e-02 -2.8071e-02\n",
      " 9.9126e-03  2.8527e-02 -4.8917e-02  ...   4.1376e-02  3.9987e-02  2.7992e-02\n",
      "-3.8930e-02  5.5090e-02  3.1780e-02  ...  -1.1472e-02  4.4872e-02 -3.8738e-02\n",
      "[torch.cuda.FloatTensor of size 1280x320 (GPU 0)]\n",
      "), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "1.00000e-02 *\n",
      " -4.2481\n",
      " -1.6667\n",
      "  1.7912\n",
      "    ⋮   \n",
      "  3.3815\n",
      "  2.5160\n",
      "  2.8680\n",
      "[torch.cuda.FloatTensor of size 1280 (GPU 0)]\n",
      "), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "1.00000e-02 *\n",
      " -0.9513\n",
      "  0.1104\n",
      "  4.5262\n",
      "    ⋮   \n",
      "  5.5364\n",
      "  3.5111\n",
      "  4.0987\n",
      "[torch.cuda.FloatTensor of size 1280 (GPU 0)]\n",
      "), ('attention_hidden.weight', Parameter containing:\n",
      "-3.5126e-02 -3.6871e-02  3.0801e-02  ...  -3.0323e-02  3.0818e-02 -2.7522e-02\n",
      " 2.9739e-02 -1.1577e-02  1.5152e-02  ...  -1.2554e-02  2.0334e-02 -2.6987e-02\n",
      "-1.8882e-02  1.0431e-02 -5.0015e-03  ...   3.7059e-02 -2.8019e-02 -6.0376e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-3.1138e-02  1.9701e-02  1.7067e-02  ...  -3.6705e-02  1.2516e-02 -3.8328e-02\n",
      " 2.8340e-02  3.8973e-02 -1.5920e-02  ...  -9.3183e-03  9.0757e-03 -1.2020e-02\n",
      "-2.4978e-02 -3.6858e-03  1.6532e-02  ...  -2.8327e-02 -5.4966e-03  8.3789e-03\n",
      "[torch.cuda.FloatTensor of size 320x640 (GPU 0)]\n",
      "), ('attention.weight', Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      "  0.1269 -3.1073  3.7937  2.0690  5.4602  1.8292 -3.8521  3.9692  4.3748  5.4984\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      " -0.8441 -0.0511  1.4993  1.5617  1.5707 -3.4132 -1.2649  2.3827 -1.5899 -5.1046\n",
      "\n",
      "Columns 20 to 29 \n",
      "1.00000e-02 *\n",
      " -0.5284  3.3143 -2.3046  1.4209 -1.8241  1.8237  2.7080  2.0939  2.6615 -0.2716\n",
      "\n",
      "Columns 30 to 39 \n",
      "1.00000e-02 *\n",
      "  2.0892  3.1326  0.2421  3.9808 -3.1550 -5.1762 -0.9334  4.4994  0.5902  3.4952\n",
      "\n",
      "Columns 40 to 49 \n",
      "1.00000e-02 *\n",
      "  4.8815  4.3595 -3.6237 -1.8807  2.6527  3.0322  2.4488 -0.7572  1.0390 -0.5381\n",
      "\n",
      "Columns 50 to 59 \n",
      "1.00000e-02 *\n",
      "  0.4721  3.7907 -2.2928 -4.5443 -0.6033  5.1226 -3.9474 -0.0352  0.0174  1.6376\n",
      "\n",
      "Columns 60 to 69 \n",
      "1.00000e-02 *\n",
      " -3.7461  0.7689  2.9021  4.3201  0.7484 -1.8277  1.9668 -3.9801  0.6301  0.0485\n",
      "\n",
      "Columns 70 to 79 \n",
      "1.00000e-02 *\n",
      " -2.8418  0.9349  2.9792  1.1861 -4.5810  3.6459  3.9282 -5.1998  4.8018  0.3342\n",
      "\n",
      "Columns 80 to 89 \n",
      "1.00000e-02 *\n",
      " -1.0834  2.0015  3.0392 -4.6798 -4.3654  2.6386  4.7962 -0.9891 -3.3220 -5.4508\n",
      "\n",
      "Columns 90 to 99 \n",
      "1.00000e-02 *\n",
      " -4.0887 -0.2177  1.3531 -3.4047 -4.8941  1.0373  3.8783 -4.4989  4.8175 -3.1570\n",
      "\n",
      "Columns 100 to 109 \n",
      "1.00000e-02 *\n",
      " -1.6350  0.3197  5.4890 -4.4890 -2.3673  1.5289  0.6816 -1.0908  2.5698  3.8856\n",
      "\n",
      "Columns 110 to 119 \n",
      "1.00000e-02 *\n",
      " -2.1810  3.7654 -5.4829 -2.3371  4.2794  2.5188 -1.8612 -1.8934 -2.8524 -3.8256\n",
      "\n",
      "Columns 120 to 129 \n",
      "1.00000e-02 *\n",
      "  1.5814 -1.2278  4.0358  5.5816 -2.1202 -3.4081 -4.2434 -0.8314  2.6301  1.6014\n",
      "\n",
      "Columns 130 to 139 \n",
      "1.00000e-02 *\n",
      "  4.0626  3.1773  1.6200 -1.2008 -1.0732  3.6382 -4.1999 -2.9894 -0.5382 -2.1065\n",
      "\n",
      "Columns 140 to 149 \n",
      "1.00000e-02 *\n",
      " -5.5718 -0.4072 -1.3046  0.0777 -1.7349 -2.6381 -3.5029  0.1918 -4.2570  2.7079\n",
      "\n",
      "Columns 150 to 159 \n",
      "1.00000e-02 *\n",
      "  1.5384 -3.7490  4.7539  5.2250  0.0092 -3.9551 -3.3958 -1.3421 -4.8090  5.5175\n",
      "\n",
      "Columns 160 to 169 \n",
      "1.00000e-02 *\n",
      " -4.7250  0.4480 -4.4306 -4.8620  4.3514  0.9470 -1.6379  1.5610 -4.9243  3.7493\n",
      "\n",
      "Columns 170 to 179 \n",
      "1.00000e-02 *\n",
      " -4.0188  1.6145  1.8063 -1.8939 -0.5787 -4.4030  1.5828  2.9366  3.6210 -5.1937\n",
      "\n",
      "Columns 180 to 189 \n",
      "1.00000e-02 *\n",
      "  2.7344  0.7941  1.1763 -0.9028  0.2738 -4.5603  0.0244  4.6253 -0.8698 -4.6788\n",
      "\n",
      "Columns 190 to 199 \n",
      "1.00000e-02 *\n",
      " -0.2243  3.5809  0.4003  2.5645  2.5034 -5.4054  1.9548 -4.0629 -0.7693  1.2088\n",
      "\n",
      "Columns 200 to 209 \n",
      "1.00000e-02 *\n",
      "  0.6004 -3.1192 -3.1090 -2.1748 -0.8020  2.8769  1.3253  0.7010 -3.6638  1.6687\n",
      "\n",
      "Columns 210 to 219 \n",
      "1.00000e-02 *\n",
      " -0.2206  2.0006  2.3793 -4.3917 -2.5258 -4.8089 -1.5548  4.3652 -0.1384  2.0518\n",
      "\n",
      "Columns 220 to 229 \n",
      "1.00000e-02 *\n",
      " -0.2098  3.5420 -2.8030 -5.2045 -3.9428 -0.8033 -3.0303  2.6269 -2.2944  3.9732\n",
      "\n",
      "Columns 230 to 239 \n",
      "1.00000e-02 *\n",
      " -1.8888 -4.7484 -1.5164 -0.0421 -2.3363  4.4116  2.6006  2.6630  2.3866 -4.5430\n",
      "\n",
      "Columns 240 to 249 \n",
      "1.00000e-02 *\n",
      "  4.2632 -1.7601  3.0304 -2.4212 -2.2306 -1.7418  4.1282  2.1044  1.2986  1.3450\n",
      "\n",
      "Columns 250 to 259 \n",
      "1.00000e-02 *\n",
      " -0.7340  0.5805 -3.1916  2.3136  2.9339 -0.7112 -3.1427 -3.5970 -2.3011 -1.5711\n",
      "\n",
      "Columns 260 to 269 \n",
      "1.00000e-02 *\n",
      " -5.0242  3.8323  1.2931  0.6256  4.2959 -1.7051  4.5521  2.3100  2.2806  0.2060\n",
      "\n",
      "Columns 270 to 279 \n",
      "1.00000e-02 *\n",
      "  2.8051 -3.8595 -0.0700  1.0520 -3.5827  3.8488  2.8995 -4.1759 -0.1028  0.8438\n",
      "\n",
      "Columns 280 to 289 \n",
      "1.00000e-02 *\n",
      " -3.9796 -4.6616 -5.3408  0.5908 -4.0624  3.8078 -0.6603  0.1353  5.1689  2.7260\n",
      "\n",
      "Columns 290 to 299 \n",
      "1.00000e-02 *\n",
      "  4.1609  4.5889 -0.0999  3.1386  4.1913  3.4519  4.3709  1.8031  1.2600 -2.9450\n",
      "\n",
      "Columns 300 to 309 \n",
      "1.00000e-02 *\n",
      "  4.4967 -4.0224 -4.6625 -1.0736  2.6609  1.7901  2.7599 -2.4145 -0.9585  2.6526\n",
      "\n",
      "Columns 310 to 319 \n",
      "1.00000e-02 *\n",
      "  4.6945 -0.0971  2.5980 -0.2622 -4.4515  3.6066  1.7661  3.7857  0.6433 -3.3372\n",
      "[torch.cuda.FloatTensor of size 1x320 (GPU 0)]\n",
      "), ('classifier.weight', Parameter containing:\n",
      "-2.2457e-02 -1.5277e-02 -2.8185e-02  ...  -4.8561e-03  6.6586e-03 -3.4309e-02\n",
      " 3.2473e-02  1.1596e-02 -2.1188e-02  ...  -9.9688e-03 -2.2497e-02  3.3799e-02\n",
      " 3.7954e-02 -1.6423e-02  3.3859e-02  ...   8.6650e-03  2.4235e-02 -1.7534e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 1.6401e-02  6.1557e-03  1.2938e-02  ...  -1.8373e-02  3.4294e-02  1.8184e-02\n",
      "-2.2087e-02  5.4512e-03  1.8140e-02  ...   5.8716e-03 -6.9185e-03  1.3220e-02\n",
      "-2.5311e-03  3.7692e-02 -3.7027e-03  ...   3.5566e-02 -1.2092e-02 -2.1676e-02\n",
      "[torch.cuda.FloatTensor of size 19x640 (GPU 0)]\n",
      "), ('classifier.bias', Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.1483\n",
      " -3.9066\n",
      "  1.4647\n",
      "  3.7879\n",
      " -1.3514\n",
      " -0.9059\n",
      " -1.4722\n",
      "  3.4545\n",
      "  2.1818\n",
      "  2.3615\n",
      " -1.2659\n",
      "  3.8948\n",
      "  3.2441\n",
      "  3.5142\n",
      " -3.0622\n",
      "  0.1374\n",
      "  0.8642\n",
      " -0.9061\n",
      "  2.5193\n",
      "[torch.cuda.FloatTensor of size 19 (GPU 0)]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "params = list(relation_extr.parameters())\n",
    "print(list(relation_extr.named_parameters()))\n",
    "# gradient clip\n",
    "torch.nn.utils.clip_grad_norm(params, 5.0)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params, lr = learning_rate, weight_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0 0.19459782507684495\n",
      "Train accuracy 0 0.19166666666666668\n",
      "Dev accuracy:  0.21875\n",
      "Test accuracy:  0.20794994479205006\n",
      "\n",
      "Train epoch 1 0.1916097491317325\n",
      "Train accuracy 1 0.23069444444444445\n",
      "Dev accuracy:  0.23625\n",
      "Test accuracy:  0.2270887007729113\n",
      "\n",
      "Train epoch 2 0.18881865683529112\n",
      "Train accuracy 2 0.27305555555555555\n",
      "Dev accuracy:  0.29875\n",
      "Test accuracy:  0.26683842473316155\n",
      "\n",
      "Train epoch 3 0.1864943109287156\n",
      "Train accuracy 3 0.3065277777777778\n",
      "Dev accuracy:  0.3025\n",
      "Test accuracy:  0.284504968715495\n",
      "\n",
      "Train epoch 4 0.18516193721029495\n",
      "Train accuracy 4 0.3275\n",
      "Dev accuracy:  0.335\n",
      "Test accuracy:  0.32057416267942584\n",
      "\n",
      "Train epoch 5 0.18309821393754747\n",
      "Train accuracy 5 0.36444444444444446\n",
      "Dev accuracy:  0.3525\n",
      "Test accuracy:  0.347442031652558\n",
      "\n",
      "Train epoch 6 0.18095912913481393\n",
      "Train accuracy 6 0.3951388888888889\n",
      "Dev accuracy:  0.36375\n",
      "Test accuracy:  0.36694884063305117\n",
      "\n",
      "Train epoch 7 0.17717202994558545\n",
      "Train accuracy 7 0.45625\n",
      "Dev accuracy:  0.45125\n",
      "Test accuracy:  0.4648509385351491\n",
      "\n",
      "Train epoch 8 0.17311651762988833\n",
      "Train accuracy 8 0.5029166666666667\n",
      "Dev accuracy:  0.46375\n",
      "Test accuracy:  0.458225984541774\n",
      "\n",
      "Train epoch 9 0.17028203788730834\n",
      "Train accuracy 9 0.5434722222222222\n",
      "Dev accuracy:  0.50375\n",
      "Test accuracy:  0.4924549135075451\n",
      "\n",
      "Train epoch 10 0.16624951706992255\n",
      "Train accuracy 10 0.6033333333333334\n",
      "Dev accuracy:  0.545\n",
      "Test accuracy:  0.5789473684210527\n",
      "\n",
      "Train epoch 11 0.16452987104654312\n",
      "Train accuracy 11 0.6359722222222223\n",
      "Dev accuracy:  0.6025\n",
      "Test accuracy:  0.6069193963930806\n",
      "\n",
      "Train epoch 12 0.16002242207527162\n",
      "Train accuracy 12 0.7061111111111111\n",
      "Dev accuracy:  0.6\n",
      "Test accuracy:  0.6139124033860875\n",
      "\n",
      "Train epoch 13 0.15852846033043333\n",
      "Train accuracy 13 0.7254166666666667\n",
      "Dev accuracy:  0.62125\n",
      "Test accuracy:  0.6330511593669489\n",
      "\n",
      "Train epoch 14 0.1567900973227289\n",
      "Train accuracy 14 0.7459722222222223\n",
      "Dev accuracy:  0.6275\n",
      "Test accuracy:  0.6323150533676849\n",
      "\n",
      "Train epoch 15 0.15490932444731395\n",
      "Train accuracy 15 0.7740277777777778\n",
      "Dev accuracy:  0.64\n",
      "Test accuracy:  0.6451969083548031\n",
      "\n",
      "Train epoch 16 0.1536746248602867\n",
      "Train accuracy 16 0.7925\n",
      "Dev accuracy:  0.63375\n",
      "Test accuracy:  0.6308428413691571\n",
      "\n",
      "Train epoch 17 0.15250152740213607\n",
      "Train accuracy 17 0.8044444444444444\n",
      "Dev accuracy:  0.64125\n",
      "Test accuracy:  0.6580787633419213\n",
      "\n",
      "Train epoch 18 0.15146394511063893\n",
      "Train accuracy 18 0.8273611111111111\n",
      "Dev accuracy:  0.635\n",
      "Test accuracy:  0.6624953993375046\n",
      "\n",
      "Train epoch 19 0.15067042274607553\n",
      "Train accuracy 19 0.8379166666666666\n",
      "Dev accuracy:  0.6525\n",
      "Test accuracy:  0.6794258373205742\n",
      "\n",
      "Train epoch 20 0.14893636600838767\n",
      "Train accuracy 20 0.8591666666666666\n",
      "Dev accuracy:  0.65\n",
      "Test accuracy:  0.6786897313213103\n",
      "\n",
      "Train epoch 21 0.14860122170713214\n",
      "Train accuracy 21 0.8680555555555556\n",
      "Dev accuracy:  0.655\n",
      "Test accuracy:  0.6845785793154214\n",
      "\n",
      "Train epoch 22 0.14734987745682399\n",
      "Train accuracy 22 0.8826388888888889\n",
      "Dev accuracy:  0.65125\n",
      "Test accuracy:  0.6820022083179977\n",
      "\n",
      "Train epoch 23 0.14661882678667704\n",
      "Train accuracy 23 0.8938888888888888\n",
      "Dev accuracy:  0.6525\n",
      "Test accuracy:  0.6864188443135811\n",
      "\n",
      "Train epoch 24 0.1463904431462288\n",
      "Train accuracy 24 0.9001388888888889\n",
      "Dev accuracy:  0.68125\n",
      "Test accuracy:  0.6963562753036437\n",
      "\n",
      "Train epoch 25 0.1454472663998604\n",
      "Train accuracy 25 0.9069444444444444\n",
      "Dev accuracy:  0.66\n",
      "Test accuracy:  0.6952521163047479\n",
      "\n",
      "Train epoch 26 0.1454195237490866\n",
      "Train accuracy 26 0.9104166666666667\n",
      "Dev accuracy:  0.66\n",
      "Test accuracy:  0.7011409642988591\n",
      "\n",
      "Train epoch 27 0.14542214847273296\n",
      "Train accuracy 27 0.91125\n",
      "Dev accuracy:  0.67\n",
      "Test accuracy:  0.6934118513065881\n",
      "\n",
      "Train epoch 28 0.14465387483437855\n",
      "Train accuracy 28 0.9190277777777778\n",
      "Dev accuracy:  0.65625\n",
      "Test accuracy:  0.6632315053367684\n",
      "\n",
      "Train epoch 29 0.14435553703043197\n",
      "Train accuracy 29 0.9241666666666667\n",
      "Dev accuracy:  0.6925\n",
      "Test accuracy:  0.708502024291498\n",
      "\n",
      "Train epoch 30 0.14417597631613413\n",
      "Train accuracy 30 0.9276388888888889\n",
      "Dev accuracy:  0.68375\n",
      "Test accuracy:  0.7007729112992271\n",
      "\n",
      "Train epoch 31 0.14392372800244224\n",
      "Train accuracy 31 0.9318055555555556\n",
      "Dev accuracy:  0.6975\n",
      "Test accuracy:  0.7077659182922341\n",
      "\n",
      "Train epoch 32 0.14345220999585256\n",
      "Train accuracy 32 0.9368055555555556\n",
      "Dev accuracy:  0.68625\n",
      "Test accuracy:  0.7140228192859772\n",
      "\n",
      "Train epoch 33 0.14351405358976788\n",
      "Train accuracy 33 0.9354166666666667\n",
      "Dev accuracy:  0.69\n",
      "Test accuracy:  0.6974604343025396\n",
      "\n",
      "Train epoch 34 0.1433354960216416\n",
      "Train accuracy 34 0.9379166666666666\n",
      "Dev accuracy:  0.6825\n",
      "Test accuracy:  0.7096061832903938\n",
      "\n",
      "Train epoch 35 0.14303677747646967\n",
      "Train accuracy 35 0.9423611111111111\n",
      "Dev accuracy:  0.65625\n",
      "Test accuracy:  0.6853146853146853\n",
      "\n",
      "Train epoch 36 0.14288170725107194\n",
      "Train accuracy 36 0.9454166666666667\n",
      "Dev accuracy:  0.68875\n",
      "Test accuracy:  0.7055576002944424\n",
      "\n",
      "Train epoch 37 0.14291717764404085\n",
      "Train accuracy 37 0.9480555555555555\n",
      "Dev accuracy:  0.6975\n",
      "Test accuracy:  0.7103422892896577\n",
      "\n",
      "Train epoch 38 0.1429557795988189\n",
      "Train accuracy 38 0.9468055555555556\n",
      "Dev accuracy:  0.67625\n",
      "Test accuracy:  0.6993006993006993\n",
      "\n",
      "Train epoch 39 0.1422853340374099\n",
      "Train accuracy 39 0.9547222222222222\n",
      "Dev accuracy:  0.695\n",
      "Test accuracy:  0.7099742362900258\n",
      "\n",
      "Train epoch 40 0.14203862455156113\n",
      "Train accuracy 40 0.9576388888888889\n",
      "Dev accuracy:  0.6825\n",
      "Test accuracy:  0.7040853882959146\n",
      "\n",
      "Train epoch 41 0.14176104022396935\n",
      "Train accuracy 41 0.9643055555555555\n",
      "Dev accuracy:  0.68625\n",
      "Test accuracy:  0.7022451232977549\n",
      "\n",
      "Train epoch 42 0.14153486801518333\n",
      "Train accuracy 42 0.9666666666666667\n",
      "Dev accuracy:  0.69\n",
      "Test accuracy:  0.7055576002944424\n",
      "\n",
      "Train epoch 43 0.1414052704970042\n",
      "Train accuracy 43 0.9697222222222223\n",
      "Dev accuracy:  0.685\n",
      "Test accuracy:  0.7129186602870813\n",
      "\n",
      "Train epoch 44 0.1412304519613584\n",
      "Train accuracy 44 0.9695833333333334\n",
      "Dev accuracy:  0.65\n",
      "Test accuracy:  0.6779536253220464\n",
      "\n",
      "Train epoch 45 0.14099311149782603\n",
      "Train accuracy 45 0.9733333333333334\n",
      "Dev accuracy:  0.6975\n",
      "Test accuracy:  0.7177033492822966\n",
      "\n",
      "Train epoch 46 0.14089313907755746\n",
      "Train accuracy 46 0.9748611111111111\n",
      "Dev accuracy:  0.68875\n",
      "Test accuracy:  0.7125506072874493\n",
      "\n",
      "Train epoch 47 0.1406465960542361\n",
      "Train accuracy 47 0.9777777777777777\n",
      "Dev accuracy:  0.7025\n",
      "Test accuracy:  0.7173352962826647\n",
      "\n",
      "Train epoch 48 0.14064301782184177\n",
      "Train accuracy 48 0.9772222222222222\n",
      "Dev accuracy:  0.6775\n",
      "Test accuracy:  0.708502024291498\n",
      "\n",
      "Train epoch 49 0.1405634225739373\n",
      "Train accuracy 49 0.9788888888888889\n",
      "Dev accuracy:  0.69375\n",
      "Test accuracy:  0.7037173352962827\n",
      "\n",
      "Train epoch 50 0.1407041558623314\n",
      "Train accuracy 50 0.9770833333333333\n",
      "Dev accuracy:  0.69125\n",
      "Test accuracy:  0.7125506072874493\n",
      "\n",
      "Train epoch 51 0.1405089184641838\n",
      "Train accuracy 51 0.98\n",
      "Dev accuracy:  0.68375\n",
      "Test accuracy:  0.7051895472948104\n",
      "\n",
      "Train epoch 52 0.14035915305217106\n",
      "Train accuracy 52 0.9813888888888889\n",
      "Dev accuracy:  0.67875\n",
      "Test accuracy:  0.7033492822966507\n",
      "\n",
      "Train epoch 53 0.14026226040389803\n",
      "Train accuracy 53 0.9829166666666667\n",
      "Dev accuracy:  0.70375\n",
      "Test accuracy:  0.7099742362900258\n",
      "\n",
      "Train epoch 54 0.14020685997274188\n",
      "Train accuracy 54 0.98375\n",
      "Dev accuracy:  0.67375\n",
      "Test accuracy:  0.7044534412955465\n",
      "\n",
      "Train epoch 55 0.14042513956626257\n",
      "Train accuracy 55 0.9826388888888888\n",
      "Dev accuracy:  0.71125\n",
      "Test accuracy:  0.7206477732793523\n",
      "\n",
      "Train epoch 56 0.14031478150023355\n",
      "Train accuracy 56 0.9822222222222222\n",
      "Dev accuracy:  0.69\n",
      "Test accuracy:  0.7011409642988591\n",
      "\n",
      "Train epoch 57 0.14020530545049242\n",
      "Train accuracy 57 0.9841666666666666\n",
      "Dev accuracy:  0.7025\n",
      "Test accuracy:  0.7180714022819286\n",
      "\n",
      "Train epoch 58 0.14013765507274203\n",
      "Train accuracy 58 0.9851388888888889\n",
      "Dev accuracy:  0.67625\n",
      "Test accuracy:  0.7070298122929702\n",
      "\n",
      "Train epoch 59 0.14022762252224816\n",
      "Train accuracy 59 0.9831944444444445\n",
      "Dev accuracy:  0.6825\n",
      "Test accuracy:  0.7051895472948104\n",
      "\n",
      "Train epoch 60 0.14015302737553914\n",
      "Train accuracy 60 0.9843055555555555\n",
      "Dev accuracy:  0.69\n",
      "Test accuracy:  0.7158630842841369\n",
      "\n",
      "Train epoch 61 0.14005449364582698\n",
      "Train accuracy 61 0.9856944444444444\n",
      "Dev accuracy:  0.68125\n",
      "Test accuracy:  0.7081339712918661\n",
      "\n",
      "Train epoch 62 0.14004997736877867\n",
      "Train accuracy 62 0.9868055555555556\n",
      "Dev accuracy:  0.68125\n",
      "Test accuracy:  0.7092381302907619\n",
      "\n",
      "Train epoch 63 0.14005457245641284\n",
      "Train accuracy 63 0.9859722222222222\n",
      "Dev accuracy:  0.6775\n",
      "Test accuracy:  0.7040853882959146\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 64 0.1400161980258094\n",
      "Train accuracy 64 0.9869444444444444\n",
      "Dev accuracy:  0.68125\n",
      "Test accuracy:  0.7026131762973868\n",
      "\n",
      "Train epoch 65 0.1399384648601214\n",
      "Train accuracy 65 0.9872222222222222\n",
      "Dev accuracy:  0.68875\n",
      "Test accuracy:  0.7059256532940743\n",
      "\n",
      "Train epoch 66 0.14005291329489813\n",
      "Train accuracy 66 0.9870833333333333\n",
      "Dev accuracy:  0.6875\n",
      "Test accuracy:  0.7011409642988591\n",
      "\n",
      "Train epoch 67 0.14015547149711186\n",
      "Train accuracy 67 0.9852777777777778\n",
      "Dev accuracy:  0.70625\n",
      "Test accuracy:  0.7051895472948104\n",
      "\n",
      "Train epoch 68 0.1399716458717982\n",
      "Train accuracy 68 0.9866666666666667\n",
      "Dev accuracy:  0.705\n",
      "Test accuracy:  0.715126978284873\n",
      "\n",
      "Train epoch 69 0.1398924806714058\n",
      "Train accuracy 69 0.9879166666666667\n",
      "Dev accuracy:  0.6725\n",
      "Test accuracy:  0.7051895472948104\n",
      "\n",
      "Train epoch 70 0.13994608011510637\n",
      "Train accuracy 70 0.9877777777777778\n",
      "Dev accuracy:  0.70625\n",
      "Test accuracy:  0.7092381302907619\n",
      "\n",
      "Train epoch 71 0.1398193806078699\n",
      "Train accuracy 71 0.9888888888888889\n",
      "Dev accuracy:  0.705\n",
      "Test accuracy:  0.7136547662863453\n",
      "\n",
      "Train epoch 72 0.13988268968131806\n",
      "Train accuracy 72 0.9880555555555556\n",
      "Dev accuracy:  0.69625\n",
      "Test accuracy:  0.7022451232977549\n",
      "\n",
      "Train epoch 73 0.13979669521252314\n",
      "Train accuracy 73 0.9894444444444445\n",
      "Dev accuracy:  0.69625\n",
      "Test accuracy:  0.7140228192859772\n",
      "\n",
      "Train epoch 74 0.13980978594885932\n",
      "Train accuracy 74 0.9894444444444445\n",
      "Dev accuracy:  0.6875\n",
      "Test accuracy:  0.7037173352962827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epoch_num):\n",
    "    epoch_loss = 0\n",
    "    tp = 0\n",
    "    batch_Xs, batch_ys = create_batches(train_input, train_output, max_batch_size)\n",
    "    for batch_X, batch_y in zip(batch_Xs, batch_ys):\n",
    "        X, Y = relation_extr.prepare_inout(batch_X, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = relation_extr(X)\n",
    "        #print(preds)\n",
    "        _, preds_Y = torch.max(preds, -1)\n",
    "        #print(preds_Y)\n",
    "        batch_size = Y.size()[0]\n",
    "        loss = loss_func(preds, Y)\n",
    "        batch_loss = loss #* batch_size / max_batch_size\n",
    "        #batch_loss = loss\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu().numpy()[0]\n",
    "        tp += (preds_Y == Y).float().sum().data.cpu().numpy()[0]\n",
    "        del X, Y, loss, batch_loss, preds, preds_Y\n",
    "    print(\"Train epoch\",i,epoch_loss/len(train_output))\n",
    "    print(\"Train accuracy\",i,tp/float(len(train_output)))\n",
    "    relation_extr.evaluatation(dev_input, dev_output,header = 'Dev')\n",
    "    relation_extr.evaluatation(test_input, test_output,header = 'Test')\n",
    "    #print(params[0])\n",
    "    \n",
    "    del epoch_loss, tp\n",
    "    \n",
    "    \n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.rand(64,10,320).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Baseline(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0]*10]*20\n",
    "print(a)\n",
    "a[0][0] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
