{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from visdom import Visdom\n",
    "viz = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import tokenizer, data_split, preprocess_dataset, create_batches\n",
    "from data import SemEval10_task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(parameters):\n",
    "    norm = 0\n",
    "    for param in parameters:\n",
    "        norm += torch.sum((param**2))\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_Model(nn.Module):\n",
    "    def __init__(self, word_vocab, label_vocab, word_emb_dim, pos_emb_dim, hidden_dim, output_dim, MAX_POS = 15, use_gpu = True):\n",
    "        super(Baseline_Model, self).__init__()\n",
    "        \n",
    "        # Set hyper parameters\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.pos_emb_dim = pos_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = word_emb_dim\n",
    "        \n",
    "        self.MAX_POS = MAX_POS\n",
    "        \n",
    "        \n",
    "        # Set options and other parameters\n",
    "        self.use_gpu = use_gpu\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        #self.pos_vocab = pos_vocab\n",
    "        \n",
    "        \n",
    "        # Free parameters for the model\n",
    "        # Initialize embeddings (Word and Position embeddings) \n",
    "        self.word_emb = nn.Embedding(len(self.word_vocab), self.word_emb_dim).cuda()\n",
    "        \n",
    "        self.pos1_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos1_emb.weight.data.uniform_(-0.00, 0.00)\n",
    "        self.pos2_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim).cuda()\n",
    "        self.pos2_emb.weight.data.uniform_(-0.00, 0.00)\n",
    "        \n",
    "        # Initialize LSTM parameters ()\n",
    "        self.rnn = nn.GRU(self.input_dim +  self.pos_emb_dim * 2, hidden_dim, bidirectional=True, batch_first = True).cuda()\n",
    "        \n",
    "        \n",
    "        # Initialize Attention parameters ()\n",
    "        self.attention_hidden = nn.Linear(hidden_dim * 2, hidden_dim,bias=False).cuda()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False).cuda()\n",
    "        \n",
    "        # Initialize Classifier parameters ()\n",
    "        #self.classifier_hidden = nn.Linear(hidden_dim * 2, hidden_dim).cuda()\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim).cuda()\n",
    "        \n",
    "        \n",
    "        self.word_emb.weight.data.copy_(word_vocab.vectors)\n",
    "    def prepare_inout(self, X, y=None):\n",
    "        sents, pos1, pos2 = list(zip(*X))\n",
    "        #sents = list(zip(*X))\n",
    "        #pos1 = datas['position_indices_1']\n",
    "        #pos2 = datas['position_indices_2']\n",
    "        \n",
    "        labels = y\n",
    "        \n",
    "        words = [ [ self.word_vocab.stoi[word] for word in sent] for sent in sents]\n",
    "        #print(words)\n",
    "        words_var = Variable(torch.LongTensor(words).cuda())\n",
    "        #print(words_var)\n",
    "        word_embeddings = self.word_emb(words_var)\n",
    "        #print(word_embedings)\n",
    "        \n",
    "        pos1 = np.array(pos1).astype('int')\n",
    "        #print(pos1)\n",
    "        pos1_var = Variable(torch.LongTensor(pos1).cuda())\n",
    "        pos1_embeddings = self.pos1_emb(pos1_var)\n",
    "        #print(pos1_var)\n",
    "        #print(pos1_embeddings)\n",
    "\n",
    "        pos2 = np.array(pos2).astype('int')\n",
    "        #print(pos2)\n",
    "        pos2_var = Variable(torch.LongTensor(pos2).cuda())\n",
    "        pos2_embeddings = self.pos2_emb(pos2_var)\n",
    "        \n",
    "        \n",
    "        inputs = {'word_embeddings': word_embeddings,\n",
    "                  'pos1_embeddings': pos1_embeddings,\n",
    "                  'pos2_embeddings': pos2_embeddings}\n",
    "        #inputs = torch.cat((word_embeddings, pos1_embeddings, pos2_embeddings),-1)\n",
    "        #print(inputs)\n",
    "        \n",
    "        \"\"\"print(labels)\n",
    "        if labels:\n",
    "            labels = [ self.label_vocab.stoi[label] - 1 for label in labels]\n",
    "            labels_var = Variable(torch.LongTensor(labels).cuda())\n",
    "            outputs = labels_var\n",
    "        else:\n",
    "            outputs = None\n",
    "        \"\"\"\n",
    "        labels = [ self.label_vocab.stoi[label] - 1 for label in labels]\n",
    "        labels_var = Variable(torch.LongTensor(labels).cuda())\n",
    "        outputs = labels_var\n",
    "        return inputs, outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, is_train = True):\n",
    "        word_embeddings = inputs['word_embeddings']\n",
    "        pos1_embeddings = inputs['pos1_embeddings']\n",
    "        pos2_embeddings = inputs['pos2_embeddings']\n",
    "        \n",
    "        # LSTM layer\n",
    "        X = torch.cat((word_embeddings, pos1_embeddings, pos2_embeddings),-1)\n",
    "        X = F.dropout(X, p=0.5, training=is_train)\n",
    "        hiddens, for_output = self.rnn(X)\n",
    "        #rev_hiddens, rev_output = self.rev_lstm(X)\n",
    "        hiddens = F.dropout(hiddens, p=0.5, training=is_train)\n",
    "        \n",
    "        # Self Attentive layer\n",
    "        att_hidden = F.tanh(self.attention_hidden(hiddens))\n",
    "        att_hidden = F.dropout(att_hidden, p=0.5, training=is_train)\n",
    "        att_scores = self.attention(att_hidden)\n",
    "        \n",
    "        attention_distrib = F.softmax(att_scores, dim = 1)\n",
    "        context_vector = F.tanh(torch.sum(hiddens * attention_distrib, dim = 1))\n",
    "\n",
    "        # Classifier\n",
    "        #context_hidden = self.classifier_hidden(context_vector)\n",
    "        #context_hidden = F.dropout(context_hidden, p=0.5, training=is_train)\n",
    "        \n",
    "        finals = F.softmax(self.classifier(context_vector), dim = 1)\n",
    "        if is_train:\n",
    "            return finals\n",
    "        else:\n",
    "            return finals, context_vector\n",
    "    \n",
    "    def evaluatation(self, input, output, demonstrate_result = True, analyze = False, header=\"\"):\n",
    "        batch_Xs, batch_ys = create_batches(input, output, 128, shuffle=False)\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        epoch_loss = 0\n",
    "        tp = 0\n",
    "        gold_answer = []\n",
    "        pred_answer = []\n",
    "        for batch_X, batch_Y in zip(batch_Xs, batch_ys):\n",
    "            X, Y = self.prepare_inout(batch_X, batch_Y)\n",
    "            preds, attention_distribs = relation_extr(X, is_train = False)\n",
    "            _, preds_Y = torch.max(preds, -1)\n",
    "            tp += (preds_Y == Y).float().sum().data.cpu().numpy()[0]\n",
    "            loss = loss_func(preds, Y)\n",
    "            epoch_loss += loss.data.cpu().numpy()[0] * len(batch_X)\n",
    "            \n",
    "            for y, preds_y in zip(Y, preds_Y):\n",
    "                y = y.data.cpu().numpy()[0] + 1\n",
    "                preds_y = preds_y.data.cpu().numpy()[0] + 1\n",
    "                \n",
    "                gold_answer.append(label_vocab.itos[y])\n",
    "                pred_answer.append(label_vocab.itos[preds_y])\n",
    "            if analyze:\n",
    "                for x, y, preds_y, pred_distirb, attention_distrib in zip(batch_X, batch_Y, preds_Y, preds, attention_distribs):\n",
    "                    input_str = \" \".join(x[0])\n",
    "                    preds_y = preds_y.data.cpu().numpy()[0] + 1\n",
    "                    preds = pred_distirb.data.cpu().numpy()\n",
    "                    \n",
    "                    print()\n",
    "                    \n",
    "                        \n",
    "                    if y != self.label_vocab.itos[preds_y]:\n",
    "                        print(\"input sentence: \" + input_str)\n",
    "                        print('answer label: ', \" \",y)\n",
    "                        print('wrong label: ', \" \",self.label_vocab.itos[preds_y])\n",
    "                        for i in range(len(preds)):\n",
    "                            print(self.label_vocab.itos[i+1],\"\\t:\\t\",preds[i])\n",
    "                        print()\n",
    "                        #print(x[0])\n",
    "                        #print(attention_distrib.data.cpu().numpy())\n",
    "                        plt.figure(figsize=(20,5))\n",
    "                        plt.xticks(range(len(x[0])), x[0],  rotation=30)\n",
    "                        plt.plot(range(len(x[0])), attention_distrib.data.cpu().numpy(), 'ro')\n",
    "                        plt.ylim(0, 1.0)\n",
    "                        plt.show()\n",
    "            del X,Y, loss, preds, attention_distribs\n",
    "            \n",
    "        if demonstrate_result:\n",
    "            #print('Avg loss: ')\n",
    "            print(header + \" accuracy:\\t%f\"%(tp/float(len(output))))\n",
    "            print(header + \" loss:    \\t%f\"%(epoch_loss/float(len(output))))\n",
    "            \n",
    "            # code for official evaluataion_code\n",
    "            fgold = open('./dataset/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/gold_answer.txt','w')\n",
    "            fpred = open('./dataset/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/pred_answer.txt','w')\n",
    "            i = 0\n",
    "            for gold, pred in zip(gold_answer, pred_answer):\n",
    "                fgold.write(\"%i\\t%s\\n\"%(i,gold))\n",
    "                fpred.write(\"%i\\t%s\\n\"%(i,pred))\n",
    "                \n",
    "                i += 1\n",
    "            #print('Macro F1-score')\n",
    "            #print('Micro F1-score')\n",
    "        \n",
    "        return {'loss': epoch_loss/float(len(output))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POS = 15\n",
    "train, dev = data_split(SemEval10_task8(sub_path='SemEval2010_task8_training/TRAIN_FILE.TXT'), test_rate = 0.1)\n",
    "test = SemEval10_task8(sub_path='SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "\n",
    "train_input, train_output = preprocess_dataset(train, MAX_POS, entity_normalize = False, directional_consideration = True)\n",
    "train_words = list(zip(*train_input))[0]\n",
    "\n",
    "dev_input, dev_output = preprocess_dataset(dev, MAX_POS, entity_normalize = False, directional_consideration = True)\n",
    "dev_words = list(zip(*dev_input))[0]\n",
    "\n",
    "test_input, test_output = preprocess_dataset(test, MAX_POS, entity_normalize = False, directional_consideration = True)\n",
    "test_words = list(zip(*test_input))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  lower=False)\n",
    "TEXT.build_vocab(train_words+test_words+dev_words, vectors=\"glove.840B.300d\")\n",
    "word_vocab = TEXT.vocab\n",
    "\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "LABEL.build_vocab(train_output+test_output+dev_output)\n",
    "label_vocab = LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_extr = torch.load(\"./model/nn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [40.0, 20.0]\n",
    "\n",
    "rel_list = list(relation_extr.label_vocab.stoi.keys())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other',\n",
       " 'Entity-Destination(e1,e2)',\n",
       " 'Cause-Effect(e2,e1)',\n",
       " 'Member-Collection(e2,e1)',\n",
       " 'Entity-Origin(e1,e2)',\n",
       " 'Message-Topic(e1,e2)',\n",
       " 'Component-Whole(e1,e2)',\n",
       " 'Component-Whole(e2,e1)',\n",
       " 'Instrument-Agency(e2,e1)',\n",
       " 'Content-Container(e1,e2)',\n",
       " 'Product-Producer(e2,e1)',\n",
       " 'Cause-Effect(e1,e2)',\n",
       " 'Product-Producer(e1,e2)',\n",
       " 'Content-Container(e2,e1)',\n",
       " 'Entity-Origin(e2,e1)',\n",
       " 'Message-Topic(e2,e1)',\n",
       " 'Instrument-Agency(e1,e2)',\n",
       " 'Member-Collection(e1,e2)',\n",
       " 'Entity-Destination(e2,e1)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning:RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "The class <e1> teacher </e1> is part of the school <e2> team </e2> , ensuring the unit is well integrated into the school , whilst still preserving the Montessori character and method of education .\n",
      "[0.0220716]\n",
      "Speculations on the causes behind the below capacity production of <e1> ethanol </e1> by the sugar <e2> factories </e2> in the state figured in the Legislative Council on Tuesday .\n",
      "[0.02140842]\n",
      "Entity-Destination(e1,e2)\n",
      "Flanking or backing <e1> rudders </e1> are used by <e2> towboats </e2> and other vessels that require a high degree of manoeuvrability .\n",
      "[0.0333387]\n",
      "I changed the length of the <e1> catapult </e1> <e2> arm </e2> and I changed the rubber band .\n",
      "[0.03571132]\n",
      "Cause-Effect(e2,e1)\n",
      "Mud-plastered walls , traditional display of art , handicrafts from all parts of the nation and the exclusivity of each theme state every year makes the Surajkund Mela stand apart in the <e1> league </e1> of various <e2> exhibitions </e2> of arts and crafts .\n",
      "[0.01828024]\n",
      "In New England in the early to mid-19th century , many cotton and <e1> textile </e1> <e2> factories </e2> employed large numbers of female adolescent workers from the New England area .\n",
      "[0.01724461]\n",
      "Member-Collection(e2,e1)\n",
      "Friends and <e1> family </e1> of <e2> inpatients </e2> at various hospitals and health care facilities are now able to send well wishes via email .\n",
      "[0.02781124]\n",
      "The company inserted <e1> needles </e1> into small lunch-size pork <e2> packs </e2> .\n",
      "[0.03481043]\n",
      "Entity-Origin(e1,e2)\n",
      "A large <e1> tsunami </e1> triggered by the <e2> earthquake </e2> spread outward from off the Sumatran coast .\n",
      "[0.02827271]\n",
      "Roadside <e1> attractions </e1> are frequently advertised with <e2> billboards </e2> to attract tourists .\n",
      "[0.03877558]\n",
      "Message-Topic(e1,e2)\n",
      "Some 9000 <e1> marines </e1> are moving into the small Afghan <e2> towns </e2> near the border to stop Taliban soldiers .\n",
      "[0.03558031]\n",
      "The commander has dragged <e1> soldiers </e1> into unnecessary <e2> battles </e2> .\n",
      "[0.04043045]\n",
      "Component-Whole(e1,e2)\n",
      "I moved the <e1> file </e1> into the <e2> folder </e2> .\n",
      "[0.03226852]\n",
      "The <e1> probe </e1> was generated from an amplified DNA <e2> fragment </e2> , using pRD136 as the template .\n",
      "[0.03192435]\n",
      "Component-Whole(e2,e1)\n",
      "The <e1> pizza </e1> was inside a <e2> trash bag </e2> and taped shut .\n",
      "[0.04062884]\n",
      "The Seattle women 's <e1> group </e1> distributes a number of small <e2> scholarships </e2> for single mothers who want to go to college .\n",
      "[0.03868321]\n",
      "Instrument-Agency(e2,e1)\n",
      "The <e1> metro line </e1> begins at an underground <e2> station </e2> in St. Stephen 's Green in the city centre .\n",
      "[0.03092528]\n",
      "Because steam <e1> locomotives </e1> included one or more steam <e2> engines </e2> , they are sometimes referred to as `` steam engines '' .\n",
      "[0.03912103]\n",
      "Content-Container(e1,e2)\n",
      "<e1> Dioxide </e1> has been released into the <e2> atmosphere </e2> .\n",
      "[0.01543581]\n",
      "The South China tigers are verging on the brink on <e1> extinction </e1> from hunting and <e2> deforestation </e2> , and these magestic animals are so close to being gone forever .\n",
      "[0.01601964]\n",
      "Product-Producer(e2,e1)\n",
      "The <e1> award </e1> has been given to the <e2> scholar </e2> who proved the mathematical theory .\n",
      "[0.03901828]\n",
      "This is from the <e1> introduction </e1> in the <e2> book </e2> `` Fountain Pens and Pencils , The Golden Age of Writing Instruments '' by George Fischler and Stuart Schneider , 1990 .\n",
      "[0.03548923]\n",
      "Cause-Effect(e1,e2)\n",
      "Pressure <e1> regulation </e1> using the <e2> valve </e2> and different amounts of fluid goes a long way in keeping your hydraulic log splitter in action .\n",
      "[0.01469899]\n",
      "Additional colors were screen printed and the <e1> book </e1> was enclosed in a mock alligator <e2> slipcase </e2> .\n",
      "[0.01605111]\n",
      "Product-Producer(e1,e2)\n",
      "A human <e1> finger </e1> was found in a <e2> can </e2> of menudo soup .\n",
      "[0.0281359]\n",
      "The bottles have leaked <e1> poison </e1> into the <e2> milk </e2> .\n",
      "[0.02907279]\n",
      "Content-Container(e2,e1)\n",
      "A <e1> tableau </e1> like this one is placed inside the king 's <e2> room </e2> and refers to court ceremonies .\n",
      "[0.00678069]\n",
      "Avian <e1> influenza </e1> is an infectious disease of birds caused by type A strains of the influenza <e2> virus </e2> .\n",
      "[0.00786627]\n",
      "Entity-Origin(e2,e1)\n",
      "The second <e1> lesson </e1> points to the <e2> predominant </e2> role of quality over quantity .\n",
      "[0.01604488]\n",
      "I welcome the <e1> news </e1> that a comprehensive <e2> agreement </e2> on bananas has now been reached .\n",
      "[0.01650297]\n",
      "Message-Topic(e2,e1)\n",
      "In the years following , Detroit 's <e1> population </e1> fell from a <e2> peak </e2> of roughly 1.8 million in 1950 to about half that number today .\n",
      "[0.01434391]\n",
      "The <e1> furnace </e1> <e2> thermostat </e2> is the nerve center of your home heating system .\n",
      "[0.01376777]\n",
      "Instrument-Agency(e1,e2)\n",
      "Bunn has recalled 35,600 single-cup pod brewers because the <e1> drawer </e1> of the <e2> coffeemaker </e2> opens unexpectedly during a brew cycle , posing a burn hazard to consumers .\n",
      "[0.00764345]\n",
      "Her <e1> children </e1> have gone into <e2> exile </e2> .\n",
      "[0.00789297]\n",
      "Member-Collection(e1,e2)\n",
      "All thirty of the patches sewn into the cloth in 1534 by the Poor Clare nuns to repair the <e1> devastation </e1> caused by the 1532 <e2> fire </e2> were removed .\n",
      "[0.01088673]\n",
      "Peter Gast `` corrected '' Nietzsche 's writings even after the <e1> philosopher </e1> 's <e2> breakdown </e2> and so without his approval .\n",
      "[0.01147239]\n",
      "Entity-Destination(e2,e1)\n",
      "We have injected the <e1> code </e1> into the native <e2> process </e2> .\n",
      "[0.0003722]\n",
      "We have injected the <e1> code </e1> into the native <e2> process </e2> .\n",
      "[0.0003722]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for rel in rel_list:\n",
    "    #if rel != 'Other':\n",
    "    #    continue\n",
    "        \n",
    "    max_batch_size = 16\n",
    "\n",
    "    h_list = []\n",
    "    o_list = []\n",
    "\n",
    "    batch_Xs, batch_ys = create_batches(train_input, train_output, max_batch_size, shuffle=False)\n",
    "    for batch_X, batch_y in zip(batch_Xs, batch_ys):\n",
    "        i, o = relation_extr.prepare_inout(batch_X, batch_y)\n",
    "        o_list += o.cpu().data.numpy().tolist()\n",
    "        o, h = relation_extr(i, is_train = False)\n",
    "        h_list += h.cpu().data.numpy().tolist()\n",
    "    \n",
    "    new_o_list = []\n",
    "    for i in o_list:\n",
    "        if i == 19:\n",
    "            print(i)\n",
    "        tmp = [0] * (len(relation_extr.label_vocab.stoi) - 1)\n",
    "        tmp[i] = 1\n",
    "        new_o_list.append(tmp)\n",
    "    \n",
    "    m = GPy.models.GPRegression(np.array(h_list),np.array(new_o_list)[:, relation_extr.label_vocab.stoi[rel] - 1].reshape([-1, 1]))\n",
    "    m.update_model(False)\n",
    "    m[:] = np.load(\"./model/GP_model_\" + rel + \".npy\")   \n",
    "    m.update_model(True)\n",
    "    \n",
    "    i_list = []\n",
    "    h_list = []\n",
    "    o_list = []\n",
    "\n",
    "    batch_Xs, batch_ys = create_batches(test_input, test_output, max_batch_size, shuffle=False)\n",
    "    for batch_X, batch_y in zip(batch_Xs, batch_ys):\n",
    "        i, o = relation_extr.prepare_inout(batch_X, batch_y)\n",
    "        i_list += batch_X[:, 0].tolist()\n",
    "        o_list += o.cpu().data.numpy().tolist()\n",
    "        o, h = relation_extr(i, is_train = False)\n",
    "        h_list += h.cpu().data.numpy().tolist()\n",
    "\n",
    "    new_o_list = []\n",
    "    for i in o_list:\n",
    "        tmp = [0] * (len(relation_extr.label_vocab.stoi) - 1)\n",
    "        tmp[i] = 1\n",
    "        new_o_list.append(tmp)\n",
    "\n",
    "    mean, var = m.predict(np.array(h_list)[:])\n",
    "    \n",
    "    var\n",
    "    print(rel)\n",
    "    print(\" \".join(i_list[np.argmax(var[np.array(new_o_list)[:, relation_extr.label_vocab.stoi[rel] - 1] == 1])]))\n",
    "    print(var[np.argmax(var[np.array(new_o_list)[:, relation_extr.label_vocab.stoi[rel] - 1] == 1])])\n",
    "    print(\" \".join(i_list[np.argmin(var[np.array(new_o_list)[:, relation_extr.label_vocab.stoi[rel] - 1] == 1])]))\n",
    "    print(var[np.argmin(var[np.array(new_o_list)[:, relation_extr.label_vocab.stoi[rel] - 1] == 1])])\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[379]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(test_input[379][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
